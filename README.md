Backend â€“ AI Chat with Qdrant, Redis & Gemini

This is the backend for the AI-powered chat application.
It integrates Qdrant (Vector DB) for semantic search, Redis for session storage, and Google Gemini API for generating responses.

ğŸš€ Tech Stack

Node.js + Express â€“ API server

Qdrant â€“ Vector database for storing embeddings

Redis â€“ Session & cache management

Google Gemini API â€“ LLM response generation

ğŸ“‚ Project Structure
backend/
â”œâ”€â”€embed_services
â”œâ”€â”€ingest
â”œâ”€â”€ scripts/
â”‚ â””â”€â”€ create_qdrant_collection.sh # Script to init collection
â”œâ”€â”€server.js #Entry point
â”œâ”€â”€ .env # Environment variables
â”œâ”€â”€ package.json
â””â”€â”€ README.md

âš™ï¸ Setup

1. Clone repo
   git clone https://github.com/thesawankumar/backend.git
   cd ai-chat-backend

2. Install dependencies
   npm install

3. Configure environment

Create a .env file:

PORT=8080

# Qdrant

QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=your-qdrant-api-key
QDRANT_COLLECTION=news_passages

# Gemini API

GEMINI_API_URL=https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent
GEMINI_API_KEY=your-gemini-api-key

# Redis

REDIS_URL=redis://localhost:6379
SESSION_TTL_SECONDS=604800

# Frontend

FRONTEND_ORIGIN=http://localhost:5173

â–¶ï¸ Running the Backend
Start server
npm run dev

Create Qdrant Collection
bash scripts/create_qdrant_collection.sh

ğŸ“¡ API Endpoints

1. Send Message
   POST /api/chat
   Content-Type: application/json
   {
   "sessionId": "abc123",
   "message": "Hello AI"
   }

â¡ï¸ Returns AI response and updates conversation history in Redis.

2. Get Session History
   GET /sessions/:id

â¡ï¸ Returns chat history for a session.

ğŸ³ Using with Docker

Start services with Docker Compose:

docker-compose up -d
